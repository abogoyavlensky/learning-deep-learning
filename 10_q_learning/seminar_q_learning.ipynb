{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Deep Q-learning\n",
    "Disclaimer: this notebook is an adopted version of the seminar of [this repository](https://github.com/yandexdataschool/Practical_RL).\n",
    "\n",
    "In this notebook you will teach a __pytorch__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in google colab uncomment these lines:\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/karfly/learning-deep-learning/master/10_q_learning/xvfb -O xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll use library `gym`. **Gym** is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `CartPole-v0` environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is like game: you push some button (perform action) and get back new frame (new state or new observation) and score (reward):\n",
    "\n",
    "* **observation** (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "* **reward** (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "* **done** (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "* **info** (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_s =  [-0.00618072 -0.17929972 -0.02283427  0.30813903]\n",
      "r =  1.0\n",
      "done =  False\n",
      "info =  {}\n"
     ]
    }
   ],
   "source": [
    "next_s, r, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(\"next_s = \", next_s)\n",
    "print(\"r = \", r)\n",
    "print(\"done = \", done)\n",
    "print(\"info = \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can get information about environment dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions =  2\n",
      "state_dim =  (4,)\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "print(\"n_actions = \", n_actions)\n",
    "print(\"state_dim = \", state_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 actions:\n",
    "1. `left`\n",
    "2. `right`\n",
    " \n",
    "4 numbers in state:\n",
    "1. `position`\n",
    "2. `velocity`\n",
    "3. `angle`\n",
    "4. `velocity at tip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Q-learning: building the network\n",
    "\n",
    "To train a neural network policy one must have a neural network policy. Let's build it.\n",
    "\n",
    "\n",
    "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring19/yet_another_week/_resource/qlearning_scheme.png)\n",
    "\n",
    "For your first run, please only use linear layers (nn.Linear) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly. \n",
    "\n",
    "Also please avoid using nonlinearities like sigmoid & tanh: agent's observations are not normalized so sigmoids may become saturated from init.\n",
    "\n",
    "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(\n",
    "    ## your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    state = torch.tensor(state[None], dtype=torch.float32)\n",
    "    q_values = network(state).detach().numpy()\n",
    "\n",
    "    # epsilon greedy strategy\n",
    "    ## your code here\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (\n",
    "    3, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert isinstance(list(network.modules(\n",
    "))[-1], nn.Linear), \"please make sure you predict q-values without nonlinearity (ignore if you know what you're doing)\"\n",
    "assert isinstance(get_action(\n",
    "    s), int), \"get_action(s) must return int, not %s. try int(action)\" % (type(get_action(s)))\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount(\n",
    "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] -\n",
    "               10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] -\n",
    "                       10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning via gradient descent\n",
    "\n",
    "We shall now train our agent's Q-function by minimizing the TD loss:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
    "\n",
    "To do so, we shall use `x.detach()` function which basically says \"consider this thing constant when doingbackprop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, n_dims=None):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot\n",
    "\n",
    "def where(cond, x_1, x_2):\n",
    "    \"\"\" helper: like np.where but in pytorch. \"\"\"\n",
    "    return (cond * x_1) + ((1-cond) * x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "    states = torch.tensor(states, dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)  # shape: [batch_size, state_size]\n",
    "    is_done = torch.tensor(is_done, dtype=torch.float32)  # shape: [batch_size]\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = ## your code here\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = torch.sum(predicted_qvalues * to_one_hot(actions, n_actions), dim=1)  # shape: [batch_size]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = ## your code here\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values =  ## your code here\n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    target_qvalues_for_actions = ## your code here\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = where(is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # mean squared error loss to minimize (don't forget to .detach())\n",
    "    loss = ## your code here\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "next_s, r, done, _ = env.step(a)\n",
    "loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert len(loss.size()) == 0, \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(network.parameters()).grad.detach().numpy() != 0), \"loss must be differentiable w.r.t. network weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)\n",
    "        next_s, r, done, _ = ## your code here\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            compute_td_loss([s], [a], [r], [next_s], [done]).backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 13.920\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 14.450\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 13.950\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 13.580\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 12.840\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 13.720\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 13.780\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 14.620\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 20.190\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 13.060\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 13.110\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 12.710\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 14.790\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 13.620\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 13.130\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 12.790\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 15.290\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 12.330\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 12.350\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 14.360\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 12.420\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 14.920\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 17.560\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 16.830\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 13.850\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 17.090\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 37.750\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 36.910\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 28.080\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 32.670\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 32.790\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 35.910\tepsilon = 0.366\n",
      "epoch #32\tmean reward = 36.320\tepsilon = 0.362\n",
      "epoch #33\tmean reward = 38.480\tepsilon = 0.359\n",
      "epoch #34\tmean reward = 40.180\tepsilon = 0.355\n",
      "epoch #35\tmean reward = 35.810\tepsilon = 0.352\n",
      "epoch #36\tmean reward = 38.950\tepsilon = 0.348\n",
      "epoch #37\tmean reward = 43.710\tepsilon = 0.345\n",
      "epoch #38\tmean reward = 41.280\tepsilon = 0.341\n",
      "epoch #39\tmean reward = 42.170\tepsilon = 0.338\n",
      "epoch #40\tmean reward = 47.340\tepsilon = 0.334\n",
      "epoch #41\tmean reward = 34.110\tepsilon = 0.331\n",
      "epoch #42\tmean reward = 52.860\tepsilon = 0.328\n",
      "epoch #43\tmean reward = 72.290\tepsilon = 0.325\n",
      "epoch #44\tmean reward = 85.110\tepsilon = 0.321\n",
      "epoch #45\tmean reward = 54.810\tepsilon = 0.318\n",
      "epoch #46\tmean reward = 48.640\tepsilon = 0.315\n",
      "epoch #47\tmean reward = 59.650\tepsilon = 0.312\n",
      "epoch #48\tmean reward = 94.400\tepsilon = 0.309\n",
      "epoch #49\tmean reward = 100.950\tepsilon = 0.306\n",
      "epoch #50\tmean reward = 104.180\tepsilon = 0.303\n",
      "epoch #51\tmean reward = 81.170\tepsilon = 0.299\n",
      "epoch #52\tmean reward = 127.020\tepsilon = 0.296\n",
      "epoch #53\tmean reward = 98.470\tepsilon = 0.294\n",
      "epoch #54\tmean reward = 115.120\tepsilon = 0.291\n",
      "epoch #55\tmean reward = 138.960\tepsilon = 0.288\n",
      "epoch #56\tmean reward = 162.630\tepsilon = 0.285\n",
      "epoch #57\tmean reward = 121.600\tepsilon = 0.282\n",
      "epoch #58\tmean reward = 157.320\tepsilon = 0.279\n",
      "epoch #59\tmean reward = 168.980\tepsilon = 0.276\n",
      "epoch #60\tmean reward = 12.550\tepsilon = 0.274\n",
      "epoch #61\tmean reward = 24.770\tepsilon = 0.271\n",
      "epoch #62\tmean reward = 11.960\tepsilon = 0.268\n",
      "epoch #63\tmean reward = 34.970\tepsilon = 0.265\n",
      "epoch #64\tmean reward = 76.190\tepsilon = 0.263\n",
      "epoch #65\tmean reward = 84.780\tepsilon = 0.260\n",
      "epoch #66\tmean reward = 154.300\tepsilon = 0.258\n",
      "epoch #67\tmean reward = 244.170\tepsilon = 0.255\n",
      "epoch #68\tmean reward = 99.060\tepsilon = 0.252\n",
      "epoch #69\tmean reward = 40.150\tepsilon = 0.250\n",
      "epoch #70\tmean reward = 84.130\tepsilon = 0.247\n",
      "epoch #71\tmean reward = 146.070\tepsilon = 0.245\n",
      "epoch #72\tmean reward = 161.260\tepsilon = 0.242\n",
      "epoch #73\tmean reward = 148.720\tepsilon = 0.240\n",
      "epoch #74\tmean reward = 228.850\tepsilon = 0.238\n",
      "epoch #75\tmean reward = 111.840\tepsilon = 0.235\n",
      "epoch #76\tmean reward = 20.020\tepsilon = 0.233\n",
      "epoch #77\tmean reward = 79.600\tepsilon = 0.231\n",
      "epoch #78\tmean reward = 120.560\tepsilon = 0.228\n",
      "epoch #79\tmean reward = 112.860\tepsilon = 0.226\n",
      "epoch #80\tmean reward = 85.480\tepsilon = 0.224\n",
      "epoch #81\tmean reward = 236.000\tepsilon = 0.222\n",
      "epoch #82\tmean reward = 26.470\tepsilon = 0.219\n",
      "epoch #83\tmean reward = 65.890\tepsilon = 0.217\n",
      "epoch #84\tmean reward = 153.880\tepsilon = 0.215\n",
      "epoch #85\tmean reward = 51.450\tepsilon = 0.213\n",
      "epoch #86\tmean reward = 89.070\tepsilon = 0.211\n",
      "epoch #87\tmean reward = 165.350\tepsilon = 0.209\n",
      "epoch #88\tmean reward = 228.100\tepsilon = 0.206\n",
      "epoch #89\tmean reward = 194.520\tepsilon = 0.204\n",
      "epoch #90\tmean reward = 143.900\tepsilon = 0.202\n",
      "epoch #91\tmean reward = 155.990\tepsilon = 0.200\n",
      "epoch #92\tmean reward = 119.310\tepsilon = 0.198\n",
      "epoch #93\tmean reward = 62.450\tepsilon = 0.196\n",
      "epoch #94\tmean reward = 39.890\tepsilon = 0.194\n",
      "epoch #95\tmean reward = 37.830\tepsilon = 0.192\n",
      "epoch #96\tmean reward = 39.410\tepsilon = 0.191\n",
      "epoch #97\tmean reward = 41.090\tepsilon = 0.189\n",
      "epoch #98\tmean reward = 44.400\tepsilon = 0.187\n",
      "epoch #99\tmean reward = 54.470\tepsilon = 0.185\n",
      "epoch #100\tmean reward = 70.400\tepsilon = 0.183\n",
      "epoch #101\tmean reward = 113.000\tepsilon = 0.181\n",
      "epoch #102\tmean reward = 126.320\tepsilon = 0.179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-e9b71d6b012d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     session_rewards = [generate_session(\n\u001b[0;32m----> 3\u001b[0;31m         epsilon=epsilon, train=True) for _ in range(100)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(\n\u001b[1;32m      5\u001b[0m         i, np.mean(session_rewards), epsilon))\n",
      "\u001b[0;32m<ipython-input-77-e9b71d6b012d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     session_rewards = [generate_session(\n\u001b[0;32m----> 3\u001b[0;31m         epsilon=epsilon, train=True) for _ in range(100)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(\n\u001b[1;32m      5\u001b[0m         i, np.mean(session_rewards), epsilon))\n",
      "\u001b[0;32m<ipython-input-76-6ef1d47604dc>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max, epsilon, train)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret results\n",
    "\n",
    "\n",
    "Welcome to the f world of deep f reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
    "\n",
    "Seriously though,\n",
    "* **mean reward** is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture. \n",
    "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
    "* **epsilon** - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record videos\n",
    "\n",
    "As usual, we now use `gym.wrappers.Monitor` to record a video of our agent playing the game. Unlike our previous attempts with state binarization, this time we expect our agent to act ~~(or fail)~~ more smoothly since there's no more binarization error at play.\n",
    "\n",
    "As you already did with tabular q-learning, we set epsilon=0 for final evaluation to prevent agent from exploring himself to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
    "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.5.25047.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
