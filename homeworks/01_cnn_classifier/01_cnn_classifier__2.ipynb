{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "- accuracy more than 60%\n",
    "- LR schedule\n",
    "- tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "len(train_dataset) = 50000\n",
      "Files already downloaded and verified\n",
      "len(val_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 4\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"len(train_dataset) =\", len(train_dataset))\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataloader= torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"len(val_dataset) =\", len(val_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape = torch.Size([4, 3, 32, 32])\n",
      "labels = tensor([1, 2, 7, 1])\n",
      "labels.shape = torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  car  bird horse   car\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    img = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "print(\"images.shape =\", images.shape)\n",
    "\n",
    "print(\"labels =\", labels)\n",
    "print(\"labels.shape =\", labels.shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.l1 = nn.Linear(16 * 8 * 8, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 16 * 8 * 8)\n",
    "        \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))  \n",
    "        x = self.l3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleConvNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# learning_rate = 1e-4\n",
    "# opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "experiment_title = \"cnn\"\n",
    "\n",
    "experiment_name = \"{}@{}\".format(experiment_title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"./tb\", experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a77c48924d0405b9dc16098b9363b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 0] loss: 2.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3e126585f4458ab1338cdbda3e3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] loss: 2.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d2f3d5df240ecb408178b2cbd23d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 2] loss: 2.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c83149d9ba47ab810b9fdcab753d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 3] loss: 2.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8470cd4865bd417097204692355152ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 4] loss: 2.31\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "n_iters_total = 0\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # dump statistics\n",
    "        writer.add_scalar(\"train/loss\", loss.item(), global_step=n_iters_total)\n",
    "        n_iters_total += 1\n",
    "        \n",
    "        \n",
    "    print(\"[epoch {}] loss: {:.3}\".format(epoch, running_loss / len(train_dataloader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3663b735f7e9404da6acb489b2b0e6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the network on the 10000 val images: 9.98%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        outputs = model(image_batch)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == label_batch).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the 10000 val images: {:.4}%\".format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "len(train_dataset) = 50000\n",
      "Files already downloaded and verified\n",
      "len(val_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"len(train_dataset) =\", len(train_dataset))\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataloader= torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"len(val_dataset) =\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.alexnet(pretrained=True).to(device)\n",
    "# model = torchvision.models.vgg11(pretrained=True).to(device)\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over parameters and freeze them\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "opt = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c316e91f3314dd293b07723923d94cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b4df42210158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "## you code here (you can just copy-paste from code above)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    print(\"[epoch {}] loss: {:.3}\".format(epoch, running_loss / len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you code here (you can just copy-paste from code above)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        outputs = model(image_batch)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == label_batch).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the 10000 val images: {:.4}%\".format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
