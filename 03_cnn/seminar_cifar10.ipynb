{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training CIFAR10 classifier\n",
    "Disclaimer: This notebook is an adopted version of [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "<img src=\"static/cifar10.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! You have seen how to define neural networks, compute loss and make updates to the weights of the network. Today we'll train CIFAR10 classifier in 2 modes:\n",
    "1. train model from scratch\n",
    "2. finetune pretrained model\n",
    "\n",
    "But before start, let's understand what CIFAR10 dataset is.\n",
    "\n",
    "**CIFAR10** - 10-class dataset of 32x32 RGB images. The main difference from MNIST is that in CIFAR10 images have 3 channels (RGB). CIFAR10 images are downscaled photos of **real** objects:  `airplane`, `automobile`, `bird`, `cat`, `deer`.\n",
    "`dog`, `frog`, `horse`, `ship`, `truck`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (2 points). Train model from scratch\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load and normalize the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and normalizing CIFAR10\n",
    "Using ``torchvision``, itâ€™s extremely easy to load CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range ~[-1, 1]. This weird numbers are just means and stds calculated on ImageNet (check [this docs](https://pytorch.org/docs/master/torchvision/models.html) for more information). **Why should we normalize images?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup dataloaders (downloading CIFAR10 can take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "len(train_dataset) = 50000\n",
      "Files already downloaded and verified\n",
      "len(val_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"len(train_dataset) =\", len(train_dataset))\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataloader= torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"len(val_dataset) =\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show some of the training images, for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape = torch.Size([4, 3, 32, 32])\n",
      "labels = tensor([7, 1, 0, 3])\n",
      "labels.shape = torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse   car plane   cat\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    img = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "print(\"images.shape =\", images.shape)\n",
    "\n",
    "print(\"labels =\", labels)\n",
    "print(\"labels.shape =\", labels.shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Convolutional Neural Network\n",
    "Here we'll define convolutional neural network. Below you can find scheme of network. Implement it using PyTorch:\n",
    "\n",
    "1. Conv (3 -> 6, 5x5, stride=1, padding=`same`)\n",
    "2. ReLU\n",
    "3. MaxPool (2x2, stride=2)\n",
    "4. Conv (6 -> 16, 5x5, stride=1, padding=`same`)\n",
    "5. ReLU\n",
    "6. Linear (`???` -> 128)\n",
    "7. ReLU\n",
    "8. Linear (128 -> 64)\n",
    "9. ReLU\n",
    "10. Linear (64 -> 10)\n",
    "\n",
    "Here padding=`same` means that the size of image doesn't change, so you have to calculate padding value yourself. Also you can see unknown `???` in 6th layer (calculate it yourself or find it emperically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, padding=2)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.l1 = nn.Linear(16 * 8 * 8, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 16 * 8 * 8)\n",
    "        \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))  \n",
    "        x = self.l3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Loss function and optimizer\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the network\n",
    "\n",
    "This is when things start to get interesting. We simply have to loop over our dataloader, and feed the inputs to the\n",
    "network and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc01040397f4e38919730969f131af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6f0072d70368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/learning-deep-learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-87393b540c6c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/learning-deep-learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/learning-deep-learning/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    print(\"[epoch {}] loss: {:.3}\".format(epoch, running_loss / len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the network on the validation data\n",
    "\n",
    "We have trained the network for 2 epochs over the training dataset. But we need to check if the network has learnt anything at all.\n",
    "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the validation set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(val_dataloader))\n",
    "image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(image_batch))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[label_batch[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(image_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"Predicted: {}\".format(\" \".join(classes[predicted[j]]for j in range(batch_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem pretty good.\n",
    "\n",
    "Let us look at how the network performs on the whole dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        outputs = ## your code here\n",
    "        predicted = ## your code here\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == label_batch).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the 10000 val images: {:.4}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks waaay better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something. You're awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (2 points). Finetune pretrained model\n",
    "\n",
    "As we already know, **deep learning** is about hierarchical feature learning. It means that major part of neural network just learns how to extract features, and only last layers laearn how to solve target task (e.g. classification). So if we take some modern model carefully trained on a big dataset, it's likely that it has learnt to extract some useful features from data.\n",
    "\n",
    "In this part of the seminar, we'll finetune [AlexNet](https://arxiv.org/abs/1404.5997) (one of the first deep CNN-architectures) trained on ImageNet (1000-class image dataset).\n",
    "\n",
    "We'll do following steps:\n",
    "1. Load and initialize the pretrained model\n",
    "2. Freeze part of the network responsible for extracting *features*\n",
    "3. Replace existing *classifying* part of the network with yours\n",
    "4. Finetune resulting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can read in the [AlexNet paper](https://arxiv.org/abs/1404.5997), takes images of size 224x224. Add proper resize to our transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup dataloaders (the same as before, but with different transform):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "len(train_dataset) = 50000\n",
      "Files already downloaded and verified\n",
      "len(val_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"len(train_dataset) =\", len(train_dataset))\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataloader= torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"len(val_dataset) =\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and initialize the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load already pretrained AlexNet model. Luckily `torchvision` gives us easy interface to load popular pretrained models (weights downloading can take some time): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /Users/m.bogoyavlenskaya/.torch/models/alexnet-owt-4df8aa71.pth\n",
      "244418560.0 bytes\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.alexnet(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at [source code](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) for AlexNet for deeper understanding of how model works. Let's print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, AlexNet has 2 main submodules:\n",
    " - **features**: extracts high-abstract features from input images\n",
    " - **classifier**: classifies resulting features in 1000 ImageNet classes\n",
    " \n",
    "`features` module is very useful for us, and we'll use it as it is. `classifier` module is more specific for ImageNet classes and we'll replace it with our own.\n",
    "\n",
    "We don't want to finetune `features` module, so let's freeze it by setting `requiers_grad=False` to its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate over parameters and freeze them\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our own classifier. Use [nn.Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) for simplisity. Also try using [nn.Dropout](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) just as in original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(256 * 6 * 6, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace existing classifier with ours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup criterion and optimizer.\n",
    "\n",
    "**Note**: we pass to optimizer only those parameters that we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "opt = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training for only 1 epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11bf5f085ca422390121ccdd239becf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 0] loss: 0.879\n"
     ]
    }
   ],
   "source": [
    "## you code here (you can just copy-paste from code above)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(image_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    print(\"[epoch {}] loss: {:.3}\".format(epoch, running_loss / len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec41db7a8a40a28351ba72272bab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the network on the 10000 val images: 78.78%\n"
     ]
    }
   ],
   "source": [
    "## you code here (you can just copy-paste from code above)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        image_batch, label_batch = batch\n",
    "        image_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "        \n",
    "        outputs = model(image_batch)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == label_batch).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the 10000 val images: {:.4}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! We seriously improved accuracy of our model by just finetuning several linear layers. Here is the power of finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
